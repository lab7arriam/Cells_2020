{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This file contains a few command line code pieces documented for the sake of code reproducibility. Most of the scripts, except for those used for read correction, were launched from the command line directly; respective commands are represented in the *bash*-flavored chunks. Also, several commands and as well as manual file editing lines are not documented here and are described in the manuscript and/or in the RMarkdown notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h2>08.11.18</h2></div>\n",
    "<b>22:37</b>: running <i>rcorrector.sh</i>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#! /bin/bash\n",
      "\n",
      "mkdir rcorrected\n",
      "\n",
      "for dir in $(ls trimmed_loose); do if [[ $dir == 1 || $dir == 5 ]]; then\n",
      "\n",
      "mkdir rcorrected/$dir\n",
      "\n",
      "R1=$(ls trimmed_loose/$dir | grep 'R1')\n",
      "\n",
      "R2=$(ls trimmed_loose/$dir | grep 'R2') \n",
      "\n",
      "perl ~/rcorrector/run_rcorrector.pl -1 trimmed_loose/$dir/$R1 -2 trimmed_loose/$dir/$R2 -od rcorrected/$dir -t 72\n",
      "\n",
      "fi; done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/reverend_casy/tmp_fs')\n",
    "with open('rcorrector_script.sh', 'r') as handle:\n",
    "    for line in handle.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.run('./rcorrector_script.sh', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>00:01:</b>: merging uncorrected reads:\n",
    "```\n",
    "for dir in $(ls trimmed_loose); do if [[ $dir == 1 || $dir == 5 ]]; then find ./trimmed_loose/$dir -name '*R1*' -exec cat {} >> merged_uncorrected/R1_uncorr.fastq; fi; done\n",
    "```\n",
    "The same was then performed with R2 reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>01:40:</b> correcting merged reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#! /bin/bash\n",
      "\n",
      "mkdir merged_rcorrected\n",
      "\n",
      "R1=$(ls merged_uncorrected | grep 'R1')\n",
      "\n",
      "R2=$(ls merged_uncorrected | grep 'R2') \n",
      "\n",
      "echo merged_uncorrected/$R1\n",
      "\n",
      "perl ~/rcorrector/run_rcorrector.pl -1 merged_uncorrected/$1 -2 merged_uncorrected/$R2 -od merged_rcorrected -t 72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open('rcorrector_script_2.sh', 'r', encoding='utf-8',\n",
    "                 errors='ignore') as handle:\n",
    "    for line in handle.readlines():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Jellyfish k-mer estimation</h3>\n",
    "```bash\n",
    "\n",
    "mkdir jellyfish_stats\n",
    "\n",
    "cat ../merged_uncorrected/R1_uncorr.fastq ../merged_uncorrected/R2_uncorr.fastq > uncorr.concat.fasta\n",
    "\n",
    "cat ../merged_rcorrected/R1_uncorr.cor.fq ../merged_rcorrected/R2_uncorr.cor.fq > corr.concat.fasta\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='center'><h2>09.11.19</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>16:52:</b> Running Trinity assembly on the corrected merged reads:\n",
    "```\n",
    "Trinity --seqrType fq --righ merged_rcorrected/R1_uncorr.cor.fq --right merged_rcorrected/R2_uncorr.cor.fq -max_memory 100G -CPU 72 --output trinity_merged_rcorrected_assembly\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    def create_dict(contig_list:list, protein_list:list)->dict:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# from operator import attrgetter\n",
    "from Bio import SeqIO\n",
    "\n",
    "def read_fasta(file:str)->list:\n",
    "    with open(file, 'r') as handle:\n",
    "        seq_list = []\n",
    "        for entry in SeqIO.parse(handle, 'fasta'):\n",
    "            seq_list.append(entry)\n",
    "    return seq_list\n",
    "\n",
    "def filter_empty(contig_list:list, protein_list:list)->list:\n",
    "    \"\"\"\n",
    "    wipe out contigs without any coding sequence embedded or coding sequence lost after CD-Hit clustering\n",
    "    \"\"\"\n",
    "    contig_list = [contig for contig in contig_list if contig.id in map(lambda x: x.id[:-3], protein_list)]\n",
    "    return contig_list\n",
    "    # works properly\n",
    "\n",
    "\n",
    "def filter_assembly(contig_list:list, protein_list:list)->list:\n",
    "    \"\"\"\n",
    "    remove contigs which fail to contain a plausible coding sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    def filter_short_contigs(contig_list:list)->list:\n",
    "        contig_list = [contig for contig in contig_list if len(contig.seq) >= 300]\n",
    "        return contig_list\n",
    "    # works properly\n",
    "    \n",
    "    def create_dict(contig_list:list, protein_list:list)->dict:\n",
    "        \"\"\"\n",
    "        devise a list dictionary to store all proteins \n",
    "        note that since SeqRecord objects are unhashable we use contig IDs as dictionary keys\n",
    "        \"\"\"\n",
    "        contig_dict = defaultdict(list)\n",
    "        for contig in contig_list:\n",
    "            for protein in protein_list:\n",
    "                if protein.id[:-3] == contig.id\n",
    "                    contig_dict[contig.id].append(protein)\n",
    "                    \n",
    "        return contig_dict\n",
    "    # works properly\n",
    "    \n",
    "    \n",
    "    def leave_longest(contig_dict:dict)->dict:\n",
    "        \"\"\"\n",
    "        leave only the longest CDS if more than one ORFs are found\n",
    "        \"\"\"\n",
    "        contig_dict = dict(zip(contig_dict.keys(),\n",
    "                           map(lambda x: max(x, key=lambda y:len(y.seq)), contig_dict.values())))\n",
    "        return contig_dict\n",
    "    # works properly\n",
    "    \n",
    "    def filter_short_orfs(contig_dict:dict)->dict:\n",
    "        \"\"\"\n",
    "        remove dictionary entries whose ORFs are shorter than 300bp\n",
    "        \"\"\"\n",
    "        contig_dict = {k: v for k, v in contig_dict.items() if len(v.seq) >= 100}\n",
    "        return contig_dict\n",
    "    # works properly\n",
    "        \n",
    "    def filter_overhangs(contig_dict:dict)->dict:\n",
    "        \"\"\"\n",
    "        remove contigs whose coding sequences cover less than 30% of their total length\n",
    "        \"\"\"\n",
    "        contig_dict = {k:v for k,v in contig_dict.items() if len(v.seq) >=\n",
    "                       len(next((x for x in contig_list if x.id == k), None).seq)*0.3}\n",
    "        return contig_dict\n",
    "    \n",
    "    \n",
    "     def create_dict(contig_list:list, protein_list:list)->dict:\n",
    "        \"\"\"\n",
    "        devise a list dictionary to store all proteins \n",
    "        note that since SeqRecord objects are unhashable we use contig IDs as dictionary keys\n",
    "        \"\"\"\n",
    "        contig_dict = defaultdict(list)\n",
    "        for contig in contig_list:\n",
    "            for protein in protein_list:\n",
    "                if protein.id[:-3] == contig.id\n",
    "                    if protein\n",
    "                    contig_dict[contig.id].append(protein)\n",
    "                    \n",
    "        return contig_dict\n",
    "    # works properly\n",
    "    \n",
    "    \n",
    "    contig_list = filter_empty(contig_list, protein_list)\n",
    "    contig_list = filter_short_contigs(contig_list)\n",
    "    contig_dict = create_dict(contig_list, protein_list)\n",
    "    contig_dict = leave_longest(contig_dict)\n",
    "    contig_dict = filter_short_orfs(contig_dict)\n",
    "    contig_dict = filter_overhangs(contig_dict)\n",
    "    \n",
    "    contigs = (contig for contig in contig_list if contig.id in contig_dict.keys())\n",
    "    \n",
    "    return contigs, contig_dict.values()\n",
    "\n",
    "def write(seqs:list, file:str):\n",
    "    with open(file, 'w') as handle:\n",
    "        SeqIO.write(seqs, handle, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_fasta(file:str)->list:\n",
    "    with open(file, 'r') as handle:\n",
    "        seq_list = []\n",
    "        for entry in SeqIO.parse(handle, 'fasta'):\n",
    "            seq_list.append(entry)\n",
    "    return seq_list\n",
    "\n",
    "\n",
    "def filter_empty(contig_list:list, protein_list:list)->list:\n",
    "    \"\"\"\n",
    "    wipe out contigs without any coding sequence embedded or coding sequence lost after CD-Hit clustering\n",
    "    \"\"\"\n",
    "    contig_list = [contig for contig in contig_list if contig.id in map(lambda x: x.id[:-3], protein_list)]\n",
    "    return contig_list\n",
    "\n",
    "\n",
    "def filter_assembly(contig_list:list, protein_list:list)->list:\n",
    "    \"\"\"\n",
    "    remove contigs which fail to contain a plausible coding sequence\n",
    "    \"\"\"\n",
    "    \n",
    "     def major_filter(contig_list:list, protein_list:list)->dict:\n",
    "        \"\"\"\n",
    "        a major filtering function replacing filter_short_contig, filter_short_orfs and filte_overhangs functions\n",
    "        \"\"\"\n",
    "        contig_dict = defaultdict(list)\n",
    "        for contig in contig_list:\n",
    "            if len(contig.seq) > 300: ## replaces filter_short_contigs in the first place\n",
    "                for protein in protein_list:\n",
    "                    if protein.id[:-3] == contig.id\n",
    "                        if len(protein.seq)*3 >= len(contig.seq)*0.3 and len(protein.seq) >= 100 \n",
    "                        ## replaces filter_short_orfs and filter_overhangs\n",
    "                        contig_dict[contig.id].append(protein)\n",
    "                    \n",
    "        return contig_dict\n",
    "        \n",
    "    def leave_longest(contig_dict:dict)->dict:\n",
    "        \"\"\"\n",
    "        leave only the longest CDS if more than one ORFs are found\n",
    "        \"\"\"\n",
    "        contig_dict = dict(zip(contig_dict.keys(),\n",
    "                           map(lambda x: max(x, key=lambda y:len(y.seq)), contig_dict.values())))\n",
    "        return contig_dict    \n",
    "    \n",
    "    contig_list = filter_empty(contig_list, protein_list)\n",
    "    contig_dict = major_filter(contig_list, protein_list)\n",
    "    contig_dict = leave_longest(contig_dict)    \n",
    "    contigs = (contig for contig in contig_list if contig.id in contig_dict.keys())\n",
    "    \n",
    "    return contigs, contig_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>05:54:</b> running TransDecoder on the obtained assembly:\n",
    "```bash\n",
    "cd transdecoder_merged_rcorrected\n",
    "TransDecoder.LongOrfs -t trinity_merged_rcorrected_assembly/Trinity.fasta\n",
    "TransDecoder/Predict -t trinity_merged_rcorrected_assembly/Trinity.fasta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>23:23:</b>filtering assembly with the <i>filter_assembly</i> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contig_list, protein_list = read_fasta('trinity_merged_rcorrected_assembly/Trinity.fasta'), read_fasta('transdecoder_merged_rcorrected/Trinity.fasta.transdecoder.pep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><h2>11.11.18</h2></div>\n",
    "<b>19:56</b>: running <i>filter_script.py</i> from the command line because of the local ssshfs mount breakdown:\n",
    "\n",
    "```bash\n",
    "chmod +x filter_script.py\n",
    "mkdir filtered\n",
    "python filter_script.py -nu trinity_merged_rcorrected_assembly/Trinity.fasta -pr transdecoder_merged_rcorrected/Trinity.fasta.transdecoder.pep -o filtered\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>06:14:</b> a resulting set of 9605 contigs and respective proteins has been obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdhit_correction(contigs:str, proteins:str, cd_output:str, contig_output:str, threshold:int)->list:\n",
    "    ## first cluster proteins in proteins.fa with CD-Hit with a provided identity threshold\n",
    "    ## then use the resulting file to /filter_empty/ onto the contigs.fa\n",
    "    ## finally, write the filtered contig list to the file with /write/ function\n",
    "    \n",
    "    DEVNULL = open(os.devnull, 'w')\n",
    "    \n",
    "    def is_tool(name)->bool:\n",
    "        \"\"\"\n",
    "        Check whether `name` is on PATH\n",
    "        \"\"\"\n",
    "\n",
    "        from distutils.spawn import find_executable\n",
    "\n",
    "        return find_executable(name) is not None\n",
    "    \n",
    "    def cd_hit(proteins:str, cd_output:str, threshold:int):\n",
    "        \"\"\"\n",
    "        run CD-Hit for protein sequences if the tool is in $PATH and write the output to the specified file\n",
    "        \"\"\"\n",
    "    \n",
    "        if is_tool('cd-hit'):\n",
    "            subprocess.run(f'cd-hit -i {proteins} -o {cd_output} -c {theshold} -n 5 -d 0 -M 0 -T 72', shell=True,\n",
    "                          stdout = DEVNULL, stderr = DEVNULL)\n",
    "            \n",
    "    def filter_missing(contigs:str, filtered_proteins:str)->list:\n",
    "        \"\"\"\n",
    "        wipe out contigs whose respective proteins were filtered on CD-Hit clustering stage\n",
    "        \"\"\"\n",
    "        \n",
    "        contig_list, protein_list = read_fasta(contigs), read_fasta(filtered_proteins)\n",
    "        contig_list = [contig for contig in contig_list if contig.id in map(lambda x: x.id[:-3], protein_list)]\n",
    "        \n",
    "        return contig_list\n",
    "    \n",
    "    cd_hit(proteins, cd_output, threshold)\n",
    "    write(filter_missing(contigs, cd_output), contig_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>06:30:</b> downloading IPG entries for hugher plants\n",
    "```bash\n",
    "\n",
    "    esearch -db ipg -query \"(Viridiplantae[Organism] OR viridiplantae[All Fields]) AND plants_and_fungi[filter]\" | efetch -format fasta > blastdb/plant_seqs.fasta\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several functions coming in handy during <i>filter_script.py</i> debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_by_length(seqs:list, cutoff:int): ### implemented in find_by_length.py\n",
    "    return (seq for seq in seqs if len(seq.seq) >= cutoff)\n",
    "\n",
    "def mean_length(seqs:list)->str:\n",
    "    shortest = len(min(seqs, key = lambda x: len(x.seq)).seq)\n",
    "    longest = len(max(seqs, key = lambda x: len(x.seq)).seq)\n",
    "    mean = sum(list(map(lambda x: len(x.seq), seqs))) / len(seqs)\n",
    "    \n",
    "    return f'The longest sequence is {longest} symbols long; the shortest sequence is {shortest} symbols long; mean sequence length across the file is {mean}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Launching of CD-Hit correction with <b>0.9</b> identity cutoff after application of the filtering script has resulted in <b>25756</b> contig-protein pairs</p><p>Estimating N50 statistics for every recorded assembly step:</p>\n",
    "\n",
    "```bash\n",
    "\n",
    "TrinityStats.pl Trinity.fasta\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Quality Control</h2>\n",
    "<h3>Nx Statistics</h3>\n",
    "<h3>ExN50 Statistics</h3>\n",
    "Using rcorrected merged reads for transcript abundance and Ex90N50 statistics:\n",
    "\n",
    "```bash\n",
    "mkdir kallisto_merged_reads\n",
    "\n",
    "align_and_estimate_abundance.pl --seqType fq --transcripts filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta--est_method kallisto --output_dir kallisto_merged_reads --thread_count 72 --prep_reference\n",
    "\n",
    "align_and_estimate_abundance.pl --seqType fq --transcripts filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta --left ../merged_rcorrected/R1_uncorr.cor.fq --right ../merged_rcorrected/R2_uncorr.cor.fq --est_method kallisto --output_dir kallisto_merged_reads --thread_count 72 --debug --trinity_mode\n",
    "\n",
    "cd kallisto_merged_reads\n",
    "\n",
    "abundance_estimates_to_matrix.pl --est_method kallisto --gene_trans_map none abundance.tsv\n",
    "\n",
    "```\n",
    "\n",
    "Did not work because of the replicate number. Using all the 8 trimmed libraries pre-aligned with <i>kallisto_script.sh</i>:\n",
    "\n",
    "```bash\n",
    "\n",
    "find kallisto_Sprint2/* -name 'abundance.tsv' -exec readlink -f {} >> abundances.tsv \\;\n",
    "\n",
    "abundance_estimates_to_matrix.pl --est_method kallisto --gene_trans_map none --name_sample_by_basedir abundances.tsv\n",
    "\n",
    "\n",
    "count_matrix_features_given_MIN_TPM_threshold.pl kallisto.isoform.TPM.not_cross_norm | tee kallisto.isoform.TPM.not_cross_norm.counts_by_min_TPM\n",
    "\n",
    "contig_ExN50_statistic.pl kallisto.isoform.TMM.EXPR.matrix ../filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta | tee ExN50.stats\n",
    "\n",
    "plot_ExN50_statistic.Rscript ExN50.stats\n",
    "```\n",
    "\n",
    "<h3> Read Representation</h3>\n",
    "Estimation of read representation in the initial and final assemblies using <b>bowtie2</b>:\n",
    "\n",
    "```bash\n",
    "mkdir QC; cd QC\n",
    "\n",
    "bowtie2-build filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta assembly_bowtie.idx\n",
    "\n",
    "bowtie2-build trinity_merged_rcorrected_assembly/Trinity.fasta raw_bowtie.idx\n",
    "\n",
    "bowtie2 align -p 10 -q --no-unal -k 20 -x assembly_bowtie.idx -1 ../../merged_rcorrected/R1_uncorr.cor.fq -2 ../../merged_rcorrected/R2_uncorr.cor.fq  \\\n",
    "     2>align_stats_assembly.txt| samtools view -@10 -Sb -o bowtie2_assembly.bam\n",
    "\n",
    "bowtie2 align -p 10 -q --no-unal -k 20 -x raw_bowtie.idx -1 ../../merged_rcorrected/R1_uncorr.cor.fq -2 ../../merged_rcorrected/R2_uncorr.cor.fq  \\\n",
    "     2>align_stats_raw.txt| samtools view -@10 -Sb -o bowtie2_raw.bam\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Alignment to the reference genome </h3>\n",
    "\n",
    "```bash\n",
    "\n",
    "mkdir ~/Ps_transcriptome/cameor_genome_assembly\n",
    "\n",
    "wget -P cameor_genome_assembly https://urgi.versailles.inra.fr/download/pea/Pisum_sativum_v1a.fa\n",
    "\n",
    "\n",
    "minimap2 -ax splice Pisum_sativum.v1a.fa ../novel_merged_assembly/filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta > aln_splice.sam ###24 unmappead reads\n",
    "\n",
    "minimap2 -ax asm5 Pisum_sativum.v1a.fa ../novel_merged_assembly/filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta > aln_asm.sam ### 1890 unmapped reads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Full-length transcript analysis using BLAST+</h3>\n",
    "The commands for BLAST-based annotation are shown below. Here we demonstrate only the quality control mediate by BLASTX output:\n",
    "\n",
    "```bash\n",
    "mkdir blast_QC; cd blast_QC\n",
    "analyze_blastPlus_topHit_coverage.pl ../blast_outputs/bestHits_blastx.outfm6 ../filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta ../blastdb/plant_seqs.fasta > blast_QC.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Transcriptome annotation </h2>\n",
    "<h3>Mercator annotation</h3>\n",
    "Protein sequences have been used as query. All possible options except for <b>CONSERVATIVE</b> and <b>IS_DNA</b> have been selected, BLAST significance cutoff is set as 80 (default value)\n",
    "<h3>BlastKOALA annotation</h3>\n",
    "Protein sequences have been used as query. Taxomomical affiliation has been set as 'Plants', search file has been set as 'family_eukaryotes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Trinotate annotation</h3>\n",
    "<h4>BLAST annotation</h4>\n",
    "\n",
    "```bash\n",
    "blastx -query ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta -db ../uniprot_sprot.pep -num_threads 72 -max_target_seqs 1 -outfmt 6 -evalue 1e-5 > blastx.outfmt6\n",
    "blastp -query ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/proteins_filtered_cdhit.fasta -db ../uniprot_sprot.pep -num_threads 72 -max_target_seqs 1 -outfmt 6 -evalue 1e-5 > blastp.outfmt6\n",
    "```\n",
    "<h4>HMMer</h4>\n",
    "\n",
    "```bash\n",
    "gunzip Pfam-A.hmm.gz\n",
    "hmmpress Pfam-A.hmm\n",
    "cd Pea_novel_merged_assembly\n",
    "hmmscan --cpu 72 --domtblout ../TrinotatePFAM.out ../Pfam-A.hmm ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/proteins_filtered_cdhit.fasta > pfam.log\n",
    "```\n",
    "<h4>SignalP</h4>\n",
    "\n",
    "```bash\n",
    "signalp -format short -fasta ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/proteins_filtered_cdhit.fasta -prefix signalp_out #launched from signalp home directory\n",
    "```\n",
    "<h4>TmHMM</h4>\n",
    "\n",
    "```bash\n",
    "tmhmm --short < ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/proteins_filtered_cdhit.fasta > tmhmm.out\n",
    "```\n",
    "<h4>Data gathering</h4>\n",
    "\n",
    "```bash\n",
    "get_Trinity_gene_to_trans_map.pl ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta > contigs_filtered_cdhit.fasta.gene_to_trans_map\n",
    "Trinotate ../Trinotate.sqlite init --gene_to_trans_map contigs_filtered_cdhit.fasta.gene_to_trans_map --transcript_fasta ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta --transdecoder_pep ~/Ps_transcriptome/filtered_cdhitted/protein_0.9/proteins_filtered_cdhit.fasta\n",
    "Trinotate ../Trinotae.sqlite LOAD_swissprot_blastp blastp.outfmt6\n",
    "Trinotate ../Trinotae.sqlite LOAD_swissprot_blastx blastx.outfmt6\n",
    "Trinotate ../Trinotate.sqlite LOAD_custom_blast --outfmt ~/Ps_transcriptome/novel_merged_assembly/blast_outputs/bestHits_blastp.outfmt6 --prog blastp --dbtype plant_seqs\n",
    "Trinotate ../Trinotate.sqlite LOAD_custom_blast --outfmt ~/Ps_transcriptome/novel_merged_assembly/blast_outputs/bestHits_blastx.outfmt6 --prog blastx --dbtype plant_seqs\n",
    "Trinotate ../Trinotate.sqlite LOAD_pfam TrinotatePFAM.out\n",
    "Trinotate ../Trinotate.sqlite LOAD_tmhmm tmhmm.out\n",
    "Trinotate ../Trinotate.sqlite LOAD_signalp signalp_out_summary.signalp5\n",
    "Trinotate ../Trinotate.sqlite report > trinotate_annotation_report.xls \n",
    "```\n",
    "\n",
    "<h4>Checking contig and protein length</h4>\n",
    "\n",
    "```bash\n",
    "./mean_lengths.py -i contigs_filtered_cdhit.fasta\n",
    "./mean_lengths.py -i proteins_filtered_cdhit.fasta\n",
    "```\n",
    "\n",
    "<h4>Checking lengths of the unblasted entries</h4>\n",
    "\n",
    "```bash\n",
    "\n",
    "awk '($7 == \"\" & $8 == \"\"){print $2}' > unmatched_contigs.txt\n",
    "awk '($7 == \"\" & $8 == \"\"){print $4}' > unmatched_proteins.txt\n",
    "./mean_lengths_local.py -i contigs_filtered_cdhit.fasta -q unmatched_contigs.txt\n",
    "./mean_lengths_local.py -i proteins_filtered_cdhit.fasta -q unmatched_proteins.txt\n",
    "\n",
    "```\n",
    "\n",
    "<h3>eggNOG</h3>\n",
    "\n",
    "```bash\n",
    "emapper.py -i ~/Ps_transcriptome/novel_merged_assembly/filtered_cdhitted/protein_0.9/proteins_filtered_cdhit.fasta\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Matching BLAST outputs to the human-readable format</h3>\n",
    "Input files had been written out from the R session to the <b>BLAST_decoding</b> directory. The script *download_BLAST_names.py* provides human-readable BLAST annotations via fetching names by NCBI accessions produced by Trinotate.\n",
    "\n",
    "```bash\n",
    "gawk -i inplace 'BEGIN{FS=OFS=\"\\t\"} {if ($1=\"\") $1=None}' BLASTX_identifiers\n",
    "./download_BLAST_names.py -i BLASTX_identifiers -o BLASTX_descriptions.txt\n",
    "```\n",
    "The same operation was performed with BLASTP outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Aligning external assembly to the Sprint-2 reference </h3>\n",
    "\n",
    "```bash\n",
    "cd ext_assembly ;\n",
    "TransDecoder.LongOrfs -t GSE72573_Trinity_uniq.fasta ; TransDecoder.Predict -t  GSE72573_Trinity_uniq.fasta \n",
    "./filter_script.py -pr  GSE72573_Trinity_uniq.fasta/transdecoder.pep -nu  GSE72573_Trinity_uniq.fasta -o .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Variant Calling</h2>\n",
    "\n",
    "```bash\n",
    "mkdir VC ; cd VC\n",
    "Trinity_gene_splice_modeler.py --trinity_fasta ~/Ps_transcriptome/novel_merged_assembly/filtered_cdhitted/protein_0.9/contigs_filtered_cdhit.fasta\n",
    "\n",
    "mkdir Zhewan-1\n",
    "\n",
    "### the following commands refer to variant calling for Zhewan-1 reads; the same pipeline has been run for Zhongwan-6 SNPs\n",
    "### note that the trimmed reads for 10 and 20 (25) days had been merged in a direction-wise manner prior to increse coverage depth\n",
    "\n",
    "~/Trinity/trinityrnaseq-Trinity-v2.8.4/Analysis/SuperTranscripts/AllelicVariants/run_variant_calling.py \\\n",
    "           --st_fa trinity_genes_fasta.fasta \\\n",
    "           --st_gtf trinity_genes.gtf \\\n",
    "           -p Zhewan1_R1_trimmed.fastq Zhewan1_R1_trimmed.fastq \\\n",
    "           -o Zhewan-1\n",
    "\n",
    "cd Zhewan1 ; mkdir manual_filtering ;  mv output.vcf manual_filtering ; cd manual_filtering\n",
    "gatk SelectVariants \\\n",
    "-V output.vcf \\\n",
    "-select-type SNP \\\n",
    "-O snps.vcf\n",
    "\n",
    "gatk SelectVariants \\\n",
    "-V output.vcf \\\n",
    "-select-type INDEL \\\n",
    "-O indels.vcf\n",
    "\n",
    "gatk VariantFiltration -V snps.vcf \\\n",
    "-filter \"QD < 2.0\" --filter-name \"QD2\" \\\n",
    "-filter \"DP < 15\" --filter-name \"DP15\" \\\n",
    "-filter \"QUAL < 30.0\" --filter-name \"QUAL30\" \\\n",
    "-filter \"SOR > 3.0\" --filter-name \"SOR3\" \\\n",
    "-filter \"FS > 30.0\" --filter-name \"FS30\" \\\n",
    "-filter \"MQ < 40.0\" --filter-name \"MQ40\" \\\n",
    "-filter \"MQRankSum < -12.5\" --filter-name \"MQRankSum-12.5\" \\\n",
    "-filter \"ReadPosRankSum < -8.0\" --filter-name \"ReadPosRankSum-8\" \\\n",
    "-O snps_filtered.vcf ; awk -F '\\t' '{if($0 ~ /\\#/) print; else if($7 == \"PASS\") print}' snps_filtered.vcf > snps_filtered_PASS_only.vcf\n",
    "\n",
    "grep -E '^##' snps_filtered_PASS_only.vcf > snps_filtered_PASS_only_head.txt\n",
    "grep -E '^TRINITY' snps_filtered_PASS_only.vcf > snps_filtered_PASS_only_headless.vcf\n",
    "\n",
    "gatk VariantFiltration -V indels.vcf \\\n",
    "-filter \"QD < 2.0\" --filter-name \"QD2\" \\\n",
    "-filter \"DP < 15\" --filter-name \"DP15\" \\\n",
    "-filter \"SOR > 3.0\" --filter-name \"SOR3\" \\\n",
    "-filter \"QUAL < 30.0\" --filter-name \"QUAL30\" \\\n",
    "-filter \"FS > 60.0\" --filter-name \"FS200\" \\\n",
    "-filter \"ReadPosRankSum < -20.0\" --filter-name \"ReadPosRankSum-20\" \\\n",
    "-O indels_filtered.vcf ; awk -F '\\t' '{if($0 ~ /\\#/) print; else if($7 == \"PASS\") print}' indels_filtered.vcf > indels_filtered_PASS_only.vcf #25819 in Zhewan-1, 13944 SNPs in Zhongwan-6\n",
    "\n",
    "./filter_SupTrans_gtf.py -e trinity_genes.vcf -vc Zhewan1/manual_filtering/snps_filtered_PASS_only_headless.vcf -o snps_reallocated_gene_to_trans_headless.vcf\n",
    "\n",
    "cat snps_filtered_PASS_only_head.txt snps_filtered_PASS_only_headless.vcf snps_reallocated_gene_to_trans_headless.vcf > snps_reallocated_gene_to_trans.vcf\n",
    "\n",
    "gatk VariantFiltration \\\n",
    "-V Zhewan1/manual_filtering/snps_reallocated_gene_to_trans.vcf \\\n",
    "-O Zhewan1/manual_filtering/snps_reallocated_gene_to_trans_isHet.vcf \\\n",
    "--genotype-filter-expression \"isHet == 1\" \\\n",
    "--genotype-filter-name \"isHetFilter\"\n",
    "\n",
    "### hetero- and homozygous variants were then selected according to this filter\n",
    "\n",
    "```\n",
    "\n",
    "<h2> Variant Annotation </h2>\n",
    "\n",
    "```bash\n",
    "python filter_gff_for_snpEff -i Trinity.fasta.transdecoder.gff3 -o Trinity.fasta.transdecoder.gff3_updated\n",
    "cp Trinity.fasta.transdecoder.gff3_updated contigs_filtered_cdhit.fasta snpEff_latest/snpEff/data/Pisum\n",
    "mv snpEff_latest/snpEff/data/Pisum/Trinity.fasta.transdecoder.gff3_updated snpEff_latest/snpEff/data/Pisum/genes.gff\n",
    "mv snpEff_latest/snpEff/data/Pisum/contigs_filtered_cdhit.fasta snpEff_latest/snpEff/data/Pisum/sequences.fa\n",
    "#manually editing snpEff.config to add Pisum database to the list of available databases\n",
    "java -jar snpEff_latest/snpEff/snpEff.jar build -gff3 Pisum\n",
    "java -Xmx4g -jar snpEff_latest/snpEff/snpEff.jar -v Pisum Zhewan1/manual_filtration/snps_reallocated_gene_to_trans_Homo.vcf > Zhewan1/manual_filtration/snps_annotated_Homo.vcf\n",
    "java -Xmx4g -jar snpEff_latest/snpEff/snpEff.jar -v Pisum Zhewan1/manual_filtration/snps_reallocated_gene_to_trans_Het.vcf > Zhewan1/manual_filtration/snps_annotated_Het.vcf\n",
    "\n",
    "```\n",
    "\n",
    "<h4>28.11.19, 07:11</h4> performing the same pipeline onto the Sprint-2 SNPs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
